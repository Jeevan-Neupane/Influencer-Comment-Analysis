{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MokbAZs_qmTQ",
        "outputId": "fabfb5c7-da82-411d-c9af-513daf36b55d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at content/; to attempt to forcibly remount, call drive.mount(\"content/\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('content/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "id": "9QieP_J0qrBd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80f5cc29-93f4-4751-a1eb-3363327ac52e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.40.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.14.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.4)\n",
            "Requirement already satisfied: emoji in /usr/local/lib/python3.10/dist-packages (2.11.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install nltk\n",
        "!pip install emoji"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "id": "NH1Fs6hxq3kW"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "id": "TZWpFQFM_yDo"
      },
      "outputs": [],
      "source": [
        "import googleapiclient.discovery\n",
        "import pandas as pd\n",
        "from googleapiclient.errors import HttpError\n",
        "from typing import Optional\n",
        "\n",
        "# g_api_key = \"AIzaSyDsIHbetelbc4XU96KoDarUSYdhMy5pfLI\"\n",
        "g_api_key=\"AIzaSyDu36yVHDf2wYOkRlD8zAKdth72O-Ej1gs\"\n",
        "# print(\"api key \", g_api_key)\n",
        "api_service_name = \"youtube\"\n",
        "api_version = \"v3\"\n",
        "DEVELOPER_KEY = g_api_key\n",
        "youtube = googleapiclient.discovery.build(\n",
        "api_service_name, api_version, developerKey=DEVELOPER_KEY)\n",
        "\n",
        "def search_videos(query, max_results=25):\n",
        "    search_response = youtube.search().list(\n",
        "    q=query,\n",
        "    type=\"video\",\n",
        "    part=\"snippet\",\n",
        "    maxResults=max_results\n",
        "    ).execute()\n",
        "\n",
        "    videos = []\n",
        "    for search_result in search_response.get(\"items\", []):\n",
        "        if search_result[\"id\"][\"kind\"] == \"youtube#video\":\n",
        "            videos.append({\n",
        "                \"title\": search_result[\"snippet\"][\"title\"],\n",
        "                \"video_id\": search_result[\"id\"][\"videoId\"],\n",
        "                \"description\": search_result[\"snippet\"][\"description\"],\n",
        "                \"thumbnail\": search_result[\"snippet\"][\"thumbnails\"][\"default\"][\"url\"]\n",
        "            })\n",
        "\n",
        "    return videos\n",
        "\n",
        "def get_replies(parent_id):  # Added video_id as an argument\n",
        "    replies = []\n",
        "    next_page_token = None\n",
        "\n",
        "    while True:\n",
        "        reply_request = youtube.comments().list(\n",
        "            part=\"snippet\",\n",
        "            parentId=parent_id,\n",
        "            textFormat=\"plainText\",\n",
        "            maxResults=100,\n",
        "            pageToken=next_page_token\n",
        "        )\n",
        "        reply_response = reply_request.execute()\n",
        "\n",
        "        for item in reply_response['items']:\n",
        "            comment = item['snippet']\n",
        "            replies.append({\n",
        "                'Timestamp': comment['publishedAt'],\n",
        "                'Username': comment['authorDisplayName'],\n",
        "                # 'VideoID': video_id,\n",
        "                'Comment': comment['textDisplay'],\n",
        "                # 'Date': comment['updatedAt'] if 'updatedAt' in comment else comment['publishedAt']\n",
        "            })\n",
        "\n",
        "        next_page_token = reply_response.get('nextPageToken')\n",
        "        if not next_page_token:\n",
        "            break\n",
        "\n",
        "    return replies\n",
        "\n",
        "# Function to get all comments (including replies) for a single video\n",
        "def get_comments_for_video(video_id):\n",
        "    all_comments = []\n",
        "    next_page_token = None\n",
        "\n",
        "    while True:\n",
        "        comment_request = youtube.commentThreads().list(\n",
        "            part=\"snippet\",\n",
        "            videoId=video_id,\n",
        "            pageToken=next_page_token,\n",
        "            textFormat=\"plainText\",\n",
        "            maxResults=100\n",
        "        )\n",
        "        comment_response = comment_request.execute()\n",
        "\n",
        "        for item in comment_response['items']:\n",
        "            top_comment = item['snippet']['topLevelComment']['snippet']\n",
        "            all_comments.append({\n",
        "                'Timestamp': top_comment['publishedAt'],\n",
        "                'Username': top_comment['authorDisplayName'],\n",
        "                # 'VideoID': video_id,  # Directly using video_id from function parameter\n",
        "                'Comment': top_comment['textDisplay'],\n",
        "                # 'Date': top_comment['updatedAt'] if 'updatedAt' in top_comment else top_comment['publishedAt']\n",
        "            })\n",
        "\n",
        "            # Fetch replies if there are any\n",
        "            if item['snippet']['totalReplyCount'] > 0:\n",
        "                all_comments.extend(get_replies(item['snippet']['topLevelComment']['id']))\n",
        "\n",
        "        next_page_token = comment_response.get('nextPageToken')\n",
        "        if not next_page_token:\n",
        "            break\n",
        "\n",
        "    return all_comments"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_vids_of_channel(channel_id, max_results = 25):\n",
        "    search_response = youtube.search().list(\n",
        "          part=\"snippet\",\n",
        "          channelId=channel_id,\n",
        "          type=\"video\",\n",
        "          maxResults=max_results\n",
        "      ).execute()\n",
        "\n",
        "    videos = []\n",
        "    for search_result in search_response.get(\"items\", []):\n",
        "        if search_result[\"id\"][\"kind\"] == \"youtube#video\":\n",
        "            videos.append({\n",
        "                \"title\": search_result[\"snippet\"][\"title\"],\n",
        "                \"video_id\": search_result[\"id\"][\"videoId\"],\n",
        "                \"description\": search_result[\"snippet\"][\"description\"],\n",
        "                \"thumbnail\": search_result[\"snippet\"][\"thumbnails\"][\"default\"][\"url\"]\n",
        "            })\n",
        "\n",
        "    return videos\n",
        "\n",
        "# get_vids_of_channel(\"UC-lHJZR3Gqxm24_Vd_AJ5Yw\")"
      ],
      "metadata": {
        "id": "FVshyXhs4iKu"
      },
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "id": "GJkefxWB_2DT"
      },
      "outputs": [],
      "source": [
        "# %%time\n",
        "# comments = get_comments_for_video(\"c3b-JASoPi0\")\n",
        "# df = pd.DataFrame(comments)\n",
        "# df"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# search_videos(\"PewDiePie\")"
      ],
      "metadata": {
        "id": "70WDiZSDXlxb"
      },
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXp5ZnLIFULU"
      },
      "source": [
        "CPU times: user 7.26 s, sys: 403 ms, total: 7.67 s\n",
        "Wall time: 4min 34s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "id": "Uxzpz_Xb_4Bf"
      },
      "outputs": [],
      "source": [
        "# df.to_csv(r\"/content/content/MyDrive/yt_analysis/mr_beast_10_min_to_escape_or_this_room_explodes.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://huggingface.co/bhadresh-savani/distilbert-base-uncased-emotion?text=I+like+you.+I+love+you\n",
        "\n",
        "\n",
        "https://huggingface.co/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertForSequenceClassification"
      ],
      "metadata": {
        "id": "wapKI3qkfcMi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YiQ0j1AK6qmz",
        "outputId": "4e7ba5f3-f8ca-4d02-84f1-284d34a1ccd5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import csv\n",
        "from transformers import pipeline\n",
        "from tqdm import tqdm\n",
        "import emoji\n",
        "import re\n",
        "import nltk\n",
        "import pandas as pd\n",
        "from nltk.corpus import words\n",
        "import torch\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "import pandas as pd\n",
        "from sklearn.cluster import KMeans\n",
        "import numpy as np\n",
        "def topicModeling(reviewText):\n",
        "    # Creating a document-term matrix using CountVectorizer\n",
        "    vectorizer = CountVectorizer(stop_words='english')\n",
        "    dtm = vectorizer.fit_transform(reviewText)\n",
        "\n",
        "    # Fitting the LDA model\n",
        "    num_topics = 1  # Here we can adjust the number of topics based on our need\n",
        "    lda = LatentDirichletAllocation(n_components=num_topics, random_state=42)\n",
        "    lda.fit(dtm)\n",
        "\n",
        "    # Displaying the top words for each topic\n",
        "    feature_names = vectorizer.get_feature_names_out()\n",
        "    topic_words = []\n",
        "\n",
        "    for topic_idx, topic in enumerate(lda.components_):\n",
        "        top_words_idx = topic.argsort()[:-50 - 1:-1]  # Displaying top 50 words for each topic\n",
        "        topic_words = [feature_names[i] for i in top_words_idx]\n",
        "\n",
        "    return topic_words\n",
        "\n",
        "\n",
        "# Download the English words corpus if you haven't already\n",
        "nltk.download('words')\n",
        "\n",
        "english_words = set(words.words())\n",
        "\n",
        "def emoji2description(text):\n",
        "  return emoji.replace_emoji(text, replace=lambda chars, data_dict: ' '.join(data_dict['en'].split('_')).strip(':'))\n",
        "\n",
        "def preprocess_text(text):\n",
        "    try:\n",
        "        text = emoji2description(text)\n",
        "        # Remove emojis\n",
        "        emoji_pattern = re.compile(\"[\"\n",
        "                                   u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                                   u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                                   u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                                   u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                                   u\"\\U00002702-\\U000027B0\"\n",
        "                                   u\"\\U000024C2-\\U0001F251\"\n",
        "                                   \"]+\", flags=re.UNICODE)\n",
        "        text = emoji_pattern.sub(r'', text)\n",
        "\n",
        "        # Remove mentioned users (@username)\n",
        "        text = re.sub(r'@\\w+', '', text)\n",
        "\n",
        "        # Remove numbers\n",
        "        text = re.sub(r'\\d+', '', text)\n",
        "\n",
        "        # Remove any remaining non-alphanumeric characters except whitespace\n",
        "        text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "\n",
        "        # Strip the text\n",
        "        text = text.strip()\n",
        "\n",
        "        # Remove non-English words\n",
        "        # english_text = ' '.join(word for word in text.split() if word.lower() in english_words)\n",
        "        # print(english_text)\n",
        "        # return english_text\n",
        "        return text\n",
        "    except Exception as e:\n",
        "        print(\"An error occurred during text preprocessing:\", e)\n",
        "        return \"\"\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, BertModel\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bhadresh-savani/distilbert-base-uncased-emotion\")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"bhadresh-savani/distilbert-base-uncased-emotion\")\n",
        "model_emb = BertModel.from_pretrained('bert-base-uncased')\n",
        "emotion_names = ['sadness', 'joy', 'love', 'anger', 'fear', 'surprise']\n",
        "\n",
        "def get_emotion_and_emb(text):\n",
        "    text = preprocess_text(text)\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\",  truncation=True, max_length=512)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits = model(**inputs).logits\n",
        "\n",
        "    predicted_class_id = logits.argmax().item()\n",
        "    input_ids = inputs['input_ids']\n",
        "    attention_mask = inputs['attention_mask']\n",
        "    # Pass the input through the BERT model\n",
        "    outputs = model_emb(input_ids, attention_mask=attention_mask)\n",
        "    # Get the pooled output (sentence embedding)\n",
        "    pooled_output = outputs.pooler_output\n",
        "    # Convert the pooled output to a NumPy array\n",
        "    sentence_embedding = pooled_output.detach().numpy()\n",
        "    return emotion_names[predicted_class_id], sentence_embedding\n",
        "\n",
        "\n",
        "\n",
        "def get_clustering_res(df):\n",
        "  # df[\"Embedding\"] = df[\"Embedding\"].apply(lambda x: [float(i) for i in x.strip('[]').split()])\n",
        "\n",
        "  # Step 2: Process embeddings to get list of floats\n",
        "  # processed_embeddings = [list(map(float, emb.split(','))) for emb in embeddings]\n",
        "  processed_embeddings = [emb.tolist() for emb in df[\"Embedding\"]]\n",
        "  # Step 3: Perform KMeans clustering with 20 clusters\n",
        "  kmeans = KMeans(n_clusters=20, random_state=42)\n",
        "  cluster_labels = kmeans.fit_predict(processed_embeddings)\n",
        "\n",
        "  # Step 4: Assign comments to centroids\n",
        "  centroids = kmeans.cluster_centers_\n",
        "  comments_by_cluster = {i: [] for i in range(20)}\n",
        "\n",
        "  for i, comment in enumerate(df[\"Comment\"]):\n",
        "      comments_by_cluster[cluster_labels[i]].append(comment)\n",
        "\n",
        "  # Step 5: Count number of comments in each cluster\n",
        "  comments_count_by_cluster = {i: len(comments_by_cluster[i]) for i in range(20)}\n",
        "  clustering_res = [{} for _ in range(20)]\n",
        "  # Printing comments in centroids and number of comments in each cluster\n",
        "  for i in range(20):\n",
        "      # print(f\"Cluster {i} (Centroid: {centroids[i]}):\")\n",
        "      # print(\"Number of comments:\", comments_count_by_cluster[i])\n",
        "      # print(\"Comments:\")\n",
        "      # print(comments_by_cluster[i])\n",
        "      # print()\n",
        "      clustering_res[i][\"no_of_comments\"] = comments_count_by_cluster[i]\n",
        "      clustering_res[i][\"comments\"] = comments_by_cluster[i]\n",
        "  return clustering_res\n",
        "\n",
        "\n",
        "# def analyze_and_save(data_file):\n",
        "#     try:\n",
        "#         df = pd.read_csv(data_file)\n",
        "#         for index, row in tqdm(df.iterrows(), total=len(df)):\n",
        "#             print(index)\n",
        "#             try:\n",
        "#                 original_text = row[\"Comment\"]\n",
        "#                 text = preprocess_text(original_text)\n",
        "#                 if text:\n",
        "#                     predicted_class_id, sentence_embedding = get_emotion_and_emb(text)\n",
        "#                     max_emotion = emotion_names[predicted_class_id]\n",
        "\n",
        "#                     # Add new columns to each row\n",
        "#                     df.at[index, \"Max_Emotion\"] = max_emotion\n",
        "#                     # df.loc[index, \"Sentence_Embedding\"] = [1, 2, 3]\n",
        "#                     print(\"Hi\")\n",
        "#                 else:\n",
        "#                     df.at[index, \"Max_Emotion\"] = \"NA\"\n",
        "#                     df.at[index, \"Sentence_Embedding\"] = \"NA\"\n",
        "\n",
        "#                 if index % 10 == 0:\n",
        "#                     print(f\"Processed {index} comments\")\n",
        "#             except Exception as e_inner:\n",
        "#                 print(f\"An error occurred for comment {index}: {e_inner}\")\n",
        "#             break\n",
        "#         # Save the updated DataFrame back to the CSV file\n",
        "#         df.to_csv(\"res1.csv\", index=False)\n",
        "#     except Exception as e_outer:\n",
        "#         print(\"An error occurred while analyzing and saving the data:\", e_outer)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "# from sklearn.cluster import KMeans\n",
        "\n",
        "# # Create a dummy DataFrame\n",
        "# dummy_data = {\n",
        "#     \"Embedding\": [np.array([0.1, 0.2, 0.3]), np.array([0.5, 0.6, 0.7]), np.array([0.9, 1.0, 1.1]), np.array([1.5, 1.6, 1.7])],\n",
        "#     \"Comment\": [\"comment 1\", \"comment 2\", \"comment 3\", \"comment 4\"]\n",
        "# }\n",
        "# dummy_df = pd.DataFrame(dummy_data)\n",
        "\n",
        "# # Define the function to get clustering results\n",
        "# # def get_clustering_res(df):\n",
        "# #     # Convert ndarray to list of lists\n",
        "# #     processed_embeddings = [emb.tolist() for emb in df[\"Embedding\"]]\n",
        "\n",
        "# #     # Perform KMeans clustering with number of clusters equal to number of samples\n",
        "# #     n_clusters = 2  # Set number of clusters to number of samples\n",
        "# #     kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
        "# #     cluster_labels = kmeans.fit_predict(processed_embeddings)\n",
        "\n",
        "# #     # Assign comments to centroids\n",
        "# #     centroids = kmeans.cluster_centers_\n",
        "# #     comments_by_cluster = {i: [] for i in range(n_clusters)}\n",
        "\n",
        "# #     for i, comment in enumerate(df[\"Comment\"]):\n",
        "# #         comments_by_cluster[cluster_labels[i]].append(comment)\n",
        "\n",
        "# #     # Count number of comments in each cluster\n",
        "# #     comments_count_by_cluster = {i: len(comments_by_cluster[i]) for i in range(n_clusters)}\n",
        "\n",
        "# #     # Construct clustering result\n",
        "# #     clustering_res = [{\"no_of_comments\": comments_count_by_cluster[i], \"comments\": comments_by_cluster[i]} for i in range(n_clusters)]\n",
        "\n",
        "# #     return clustering_res\n",
        "\n",
        "# # Test the function with the dummy DataFrame\n",
        "# result = get_clustering_res(dummy_df)\n",
        "# print(result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "cVK1DEKwhfAE",
        "outputId": "07eb2e4b-e9fb-4835-f2f1-2604933224ba"
      },
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "n_samples=4 should be >= n_clusters=20.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-117-8ea341d9dba4>\u001b[0m in \u001b[0;36m<cell line: 38>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;31m# Test the function with the dummy DataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_clustering_res\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdummy_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-116-c8df1e48f8df>\u001b[0m in \u001b[0;36mget_clustering_res\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m    113\u001b[0m   \u001b[0;31m# Step 3: Perform KMeans clustering with 20 clusters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m   \u001b[0mkmeans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKMeans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_clusters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m   \u001b[0mcluster_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkmeans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessed_embeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m   \u001b[0;31m# Step 4: Assign comments to centroids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py\u001b[0m in \u001b[0;36mfit_predict\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1031\u001b[0m             \u001b[0mIndex\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mcluster\u001b[0m \u001b[0meach\u001b[0m \u001b[0msample\u001b[0m \u001b[0mbelongs\u001b[0m \u001b[0mto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1032\u001b[0m         \"\"\"\n\u001b[0;32m-> 1033\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1034\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1035\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1424\u001b[0m         )\n\u001b[1;32m   1425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1426\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_params_vs_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1428\u001b[0m         \u001b[0mrandom_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_random_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py\u001b[0m in \u001b[0;36m_check_params_vs_input\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m   1360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_check_params_vs_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1362\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_params_vs_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault_n_init\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_algorithm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malgorithm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py\u001b[0m in \u001b[0;36m_check_params_vs_input\u001b[0;34m(self, X, default_n_init)\u001b[0m\n\u001b[1;32m    857\u001b[0m         \u001b[0;31m# n_clusters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_clusters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    860\u001b[0m                 \u001b[0;34mf\"n_samples={X.shape[0]} should be >= n_clusters={self.n_clusters}.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m             )\n",
            "\u001b[0;31mValueError\u001b[0m: n_samples=4 should be >= n_clusters=20."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.array([0.1, 0.2, 0.3]).shape"
      ],
      "metadata": {
        "id": "HW8J4UQAjWSx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# analyze_and_save(r\"/content/content/MyDrive/yt_analysis/my_vid.csv\")"
      ],
      "metadata": {
        "id": "iM68MhaswOvQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_comments_and_analyze(vid, name):\n",
        "  comments = get_comments_for_video(vid)\n",
        "  for comment in comments:\n",
        "    # print(\".\")\n",
        "    comment[\"Sentiment\"], comment[\"Embedding\"] = get_emotion_and_emb(comment[\"Comment\"])\n",
        "  df = pd.DataFrame(comments)\n",
        "  df.to_csv(f\"/content/content/MyDrive/yt_analysis/competitors/{name}.csv\", index=False)\n",
        "  df.to_json(f\"/content/content/MyDrive/yt_analysis/competitors/{name}.json\", orient=\"records\", date_format=\"iso\")\n",
        "  return df\n",
        "\n"
      ],
      "metadata": {
        "id": "UZnR3fw08w9A"
      },
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# vids = [\n",
        "#     {\"name\": \"announcement_how_to_youtube_course\", \"link\": \"GREE3Go9hx8\", \"time\": \"2024-06-07\"},\n",
        "#     {\"name\": \"the_malaria_miracle\", \"link\": \"GREE3Go9hx8\", \"time\": \"2024-05-03\"},\n",
        "#     {\"name\": \"2080_in_6_min\", \"link\": \"2yn585QndWQ\", \"time\": \"2024-04-26\"},\n",
        "#     {\"name\": \"the_extra_ordinary_life_of_pushkar_shah\", \"link\": \"fbOoqRdP_bM\", \"time\": \"2024-04-19\"}\n",
        "# ]\n",
        "vids= [\n",
        "    {\"name\": \"if_i_started_a_yt_channel_in_24\", \"link\":\"T2M9hSswlIs\"}\n",
        "]\n",
        "def analyze_batch(vids):\n",
        "  topics = [{} for _ in range(len(vids))]\n",
        "  clustering_res = [{} for _ in range(len(vids))]\n",
        "  for i, vid in enumerate(vids):\n",
        "    comments = get_comments_and_analyze(vid[\"link\"], vid[\"name\"])\n",
        "    topic = topicModeling(comments[\"Comment\"].tolist())\n",
        "    topics[i][\"name\"] = vid[\"name\"]\n",
        "    topics[i][\"topics\"] = topic\n",
        "    df_ = pd.read_csv(f\"/content/content/MyDrive/yt_analysis/competitors/{vid['name']}.csv\")\n",
        "    df_['Embedding'] = df_['Embedding'].apply(lambda x: np.array([float(i) for i in x.strip('[]').split()]))\n",
        "\n",
        "    cluster = get_clustering_res(df_)\n",
        "    clustering_res[i][\"name\"] = vid[\"name\"]\n",
        "    clustering_res[i][\"cluster_info\"] = cluster\n",
        "    print(f\"{i} vids processed\")\n",
        "\n",
        "\n",
        "  topic_df = pd.DataFrame({\"topics\": topics})\n",
        "  topic_df.to_json(r\"/content/content/MyDrive/yt_analysis/competitors/topics.json\", orient=\"records\", date_format=\"iso\")\n",
        "  cluster_df = pd.DataFrame({\"clusters\": clustering_res})\n",
        "  cluster_df.to_json(r\"/content/content/MyDrive/yt_analysis/competitors/clusters.json\", orient=\"records\", date_format=\"iso\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KfX4chCOGN4-"
      },
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "analyze_batch(vids)"
      ],
      "metadata": {
        "id": "Nfs7HTXMfVjQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# comments = get_comments_and_analyze(\"1LkQQIC9QkI\")"
      ],
      "metadata": {
        "id": "ESlbB8SSAwMp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df = pd.DataFrame(comments)"
      ],
      "metadata": {
        "id": "r5FtZq66ELwu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df.head()"
      ],
      "metadata": {
        "id": "AenvbEhcERZN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df.to_json(r\"/content/content/MyDrive/yt_analysis/the_nepali_comment/biggest_plane_hijack_from_nepal.json\", orient=\"records\", date_format=\"iso\")"
      ],
      "metadata": {
        "id": "zGxXiX5-ElYv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# topics = topicModeling(df[\"Comment\"].tolist())\n",
        "# # vulgar_words = [\"unknown\", \"dhoti\", \"\"]\n",
        "# topics"
      ],
      "metadata": {
        "id": "85nVqi00IJdR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# programiz = pd.read_csv(r\"/content/content/MyDrive/yt_analysis/my_vid.csv\")\n",
        "# df_read['Embedding'] = df_read['vector_column'].apply(lambda x: [float(i) for i in x.strip('[]').split()])\n",
        "\n",
        "# programiz.head(100)"
      ],
      "metadata": {
        "id": "jQ9TiH2wx3zJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def convert_to_variable(string):\n",
        "#     # Remove special characters and spaces, and split the string into words\n",
        "#     words = string.replace('|', '').replace(' ', '_').split()\n",
        "\n",
        "#     # Convert the first letter of each word to uppercase\n",
        "#     capitalized_words = [word.capitalize() for word in words]\n",
        "\n",
        "#     # Join the words together to form the variable name\n",
        "#     variable_name = ''.join(capitalized_words)\n",
        "\n",
        "#     return variable_name\n",
        "\n",
        "# # Example usage:\n",
        "# # string = \"#1:Getting Started with C Programming | C Programming for Beginners\"\n",
        "# # variable_name = convert_to_variable(string)\n",
        "# # print(variable_name)  # Output: Getting_Started_With_C_Programming_C_Programming_For_Beginners\n"
      ],
      "metadata": {
        "id": "S75HWuugpOWp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v4MabzrH74HT"
      },
      "outputs": [],
      "source": [
        "# analyze_and_save(r\"/content/content/MyDrive/yt_analysis/mr_beast_7_days_stranded_on_island.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JnEJPIlxA2UF"
      },
      "source": [
        "Sushant KC - Sarangi (Official Music Video)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oAXNXoJG8TK9"
      },
      "outputs": [],
      "source": [
        "# comments = get_comments_for_video(\"Sh8ZYHnb86c\")\n",
        "# df = pd.DataFrame(comments)\n",
        "# df.to_csv(r\"/content/content/MyDrive/yt_analysis/sarangi.csv\", index = None)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# comments = get_comments_for_video(\"KnvbUiSxvbM\")\n",
        "# df = pd.DataFrame(comments)\n",
        "# df.to_csv(r\"/content/content/MyDrive/yt_analysis/getting_started_with_c_programming__c_programming_for_beginners.csv\", index = None)"
      ],
      "metadata": {
        "id": "kLKKYl8aoyej"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y_vZDcUwBF8N"
      },
      "outputs": [],
      "source": [
        "# analyze_and_save(r\"/content/content/MyDrive/yt_analysis/sarangi.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0nEI5edF_Va"
      },
      "source": [
        "- [ ] clean comments. remove un processable. delete empty cmt\n",
        "- [ ] get teh best model for sentiment analysis\n",
        "- [ ] get sentiment analysis and save in file\n",
        "- [ ] do clustering in similar comments.\n",
        "and more methods to get insights\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# df_ = pd.read_csv(r\"/content/content/MyDrive/yt_analysis/the_nepali_comment/biggest_plane_hijack_from_nepal.csv\")\n",
        "df_['Embedding'] = df_['Embedding'].apply(lambda x: np.array([float(i) for i in x.strip('[]').split()]))\n",
        "get_clustering_res(df_)"
      ],
      "metadata": {
        "id": "mXhp3iNdjm8Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Sample vectors of length 768\n",
        "vectors = [np.random.rand(768) for _ in range(10)]  # Creating 10 random vectors as an example\n",
        "\n",
        "# Create a DataFrame with one column\n",
        "df = pd.DataFrame({'vector_column': vectors})\n",
        "\n",
        "# Save DataFrame to a CSV file\n",
        "df.to_csv('vectors.csv', index=False)\n",
        "\n",
        "# Read DataFrame back from the CSV file\n",
        "df_read = pd.read_csv('vectors.csv')\n",
        "\n",
        "# Convert the values in the 'vector_column' to lists of floats\n",
        "df_read['vector_column'] = df_read['vector_column'].apply(lambda x: [float(i) for i in x.strip('[]').split()])\n",
        "\n",
        "print(df_read.head(20))  # Print the first few rows of the DataFrame\n"
      ],
      "metadata": {
        "id": "9ikXlFNKrBTu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Clustering\n"
      ],
      "metadata": {
        "id": "ZVyHdPKTSrDa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "import numpy as np\n",
        "\n",
        "# Step 1: Extract embeddings from DataFrame\n",
        "# df = pd.read_csv(r\"/content/content/MyDrive/yt_analysis/the_nepali_comment/biggest_plane_hijack_from_nepal.csv\")\n",
        "def get_clustering_res(df):\n",
        "  df[\"Embedding\"] = df[\"Embedding\"].apply(lambda x: [float(i) for i in x.strip('[]').split()])\n",
        "\n",
        "  # Step 2: Process embeddings to get list of floats\n",
        "  # processed_embeddings = [list(map(float, emb.split(','))) for emb in embeddings]\n",
        "  processed_embeddings = df[\"Embedding\"].tolist()\n",
        "  # Step 3: Perform KMeans clustering with 20 clusters\n",
        "  kmeans = KMeans(n_clusters=20, random_state=42)\n",
        "  cluster_labels = kmeans.fit_predict(processed_embeddings)\n",
        "\n",
        "  # Step 4: Assign comments to centroids\n",
        "  centroids = kmeans.cluster_centers_\n",
        "  comments_by_cluster = {i: [] for i in range(20)}\n",
        "\n",
        "  for i, comment in enumerate(df[\"Comment\"]):\n",
        "      comments_by_cluster[cluster_labels[i]].append(comment)\n",
        "\n",
        "  # Step 5: Count number of comments in each cluster\n",
        "  comments_count_by_cluster = {i: len(comments_by_cluster[i]) for i in range(20)}\n",
        "  clustering_res = [{} for _ in range(20)]\n",
        "  # Printing comments in centroids and number of comments in each cluster\n",
        "  for i in range(20):\n",
        "      # print(f\"Cluster {i} (Centroid: {centroids[i]}):\")\n",
        "      # print(\"Number of comments:\", comments_count_by_cluster[i])\n",
        "      # print(\"Comments:\")\n",
        "      # print(comments_by_cluster[i])\n",
        "      # print()\n",
        "      clustering_res[i][\"no_of_comments\"] = comments_count_by_cluster[i]\n",
        "      clustering_res[i][\"comments\"] = comments_by_cluster[i]\n",
        "  return clustering_res\n",
        "\n",
        "clustering_res = get_clustering_res(df)\n",
        "print(clustering_res)"
      ],
      "metadata": {
        "id": "UQcd-YBwS-aH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "comments_by_cluster[0][0]"
      ],
      "metadata": {
        "id": "2KMxhC60W9gv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Generate random embeddings\n",
        "np.random.seed(42)  # for reproducibility\n",
        "data = np.random.randn(10000, 300)\n",
        "\n",
        "# Define the range of cluster numbers to test\n",
        "cluster_range = [5, 10, ]\n",
        "inertia_values = []\n",
        "\n",
        "for n_clusters in cluster_range:\n",
        "    # Create a KMeans instance with the specified number of clusters\n",
        "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
        "\n",
        "    # Fit the KMeans model to the data\n",
        "    kmeans.fit(data)\n",
        "\n",
        "    # Append the inertia value (sum of squared distances to the closest cluster center) to the list\n",
        "    inertia_values.append(kmeans.inertia_)\n",
        "\n",
        "# Plot the inertia values against the number of clusters\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(cluster_range, inertia_values, marker='o')\n",
        "plt.xlabel('Number of Clusters')\n",
        "plt.ylabel('Inertia')\n",
        "plt.title('Elbow Plot')\n",
        "plt.show()\n",
        "\n",
        "# Choose the desired number of clusters based on the elbow plot\n",
        "n_clusters = 5  # Example value, adjust as needed\n",
        "\n",
        "# Create a new KMeans instance with the chosen number of clusters\n",
        "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
        "kmeans.fit(data)\n",
        "\n",
        "# Get the cluster labels for each embedding\n",
        "labels = kmeans.labels_\n",
        "\n",
        "# Get the centroids (cluster centers)\n",
        "centroids = kmeans.cluster_centers_\n",
        "\n",
        "# Find the closest embeddings to each centroid\n",
        "closest_embeddings = []\n",
        "for centroid in centroids:\n",
        "    distances = np.linalg.norm(data - centroid, axis=1)\n",
        "    closest_idx = np.argmin(distances)\n",
        "    closest_embeddings.append(data[closest_idx])\n",
        "\n",
        "print(f\"Closest embeddings to centroids:\")\n",
        "for i, embedding in enumerate(closest_embeddings):\n",
        "    print(f\"Centroid {i}: {embedding}\")"
      ],
      "metadata": {
        "id": "wFbp_hEZSt89"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}